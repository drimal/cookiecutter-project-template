{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b3d2fe",
   "metadata": {},
   "source": [
    "# Step 4: Report & Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e95a2",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "- [Summarize here]\n",
    "\n",
    "## Decisions / Recommendations\n",
    "- [List]\n",
    "\n",
    "## Limitations\n",
    "- [Notes]\n",
    "\n",
    "## Next Steps\n",
    "- [Plan]\n",
    "\n",
    "---\n",
    "\n",
    "_This notebook can be exported to PDF/HTML for stakeholders._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12995733",
   "metadata": {},
   "source": [
    "## Prompt-Specific Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load run log\n",
    "df_runs = pd.read_csv('../results/runs/{{cookiecutter.project_id}}_runs.csv')\n",
    "\n",
    "if 'Prompt ID' in df_runs.columns:\n",
    "    summary = df_runs.groupby('Prompt ID')['Metric'].agg(['mean','max','count']).reset_index()\n",
    "    display(summary)\n",
    "\n",
    "    # Convert to Markdown table for stakeholder report\n",
    "    md_table = summary.to_markdown(index=False)\n",
    "    print('### Metrics by Prompt ID\\n')\n",
    "    print(md_table)\n",
    "else:\n",
    "    print('No Prompt ID found in run log. Add prompts to log for this section.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc07c6d",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- Identify which prompt versions perform best on average.\n",
    "- Highlight prompts with the most runs (stability of results).\n",
    "- Recommend which prompt to standardize moving forward.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
